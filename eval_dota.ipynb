{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0831be09",
   "metadata": {
    "id": "1866a12a"
   },
   "source": [
    "# Transformer Dose Calculation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07237bb3",
   "metadata": {
    "id": "EPNNAHcrOUXd"
   },
   "source": [
    "## Import libraries and define auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908536ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3451,
     "status": "ok",
     "timestamp": 1622220341066,
     "user": {
      "displayName": "Oscar Pastor Serrano",
      "photoUrl": "",
      "userId": "05646652561813059794"
     },
     "user_tz": -120
    },
    "id": "44dc2db2",
    "outputId": "1ba8f51e-9e93-411a-c432-0ecc27ff44a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 19:00:44.719606: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-06 19:00:44.719694: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-06 19:00:44.722134: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-06 19:00:44.736250: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-06 19:00:46.741707: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/hamdi/miniconda3/envs/simelectrons/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/hamdi/datagenerator/dota/./src/plot.py:221: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if i is 0:\n",
      "/home/hamdi/datagenerator/dota/./src/plot.py:234: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if i is 0:\n",
      "/home/hamdi/datagenerator/dota/./src/plot.py:243: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if i is 0:\n",
      "/home/hamdi/datagenerator/dota/./src/plot.py:253: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if i is 0:\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "from models import dota_energies\n",
    "from preprocessing import DataRescaler\n",
    "from generators import DataGenerator\n",
    "from evaluation import infer, from_file\n",
    "from plot import plot_slice, plot_beam\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c33ff8b",
   "metadata": {
    "id": "XpnpveIhrxPp"
   },
   "source": [
    "## Load hyperparameters and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50870622",
   "metadata": {
    "id": "eRlPmU_xJERL"
   },
   "outputs": [],
   "source": [
    "# Load model and data hyperparameters.\n",
    "with open(\"./hyperparam.json\", \"r\") as hfile:\n",
    "    param = json.load(hfile)\n",
    "\n",
    "# Prepare input data.\n",
    "# path = \"./data/training/\"\n",
    "# path_test = \"./data/test/\"\n",
    "# path_weights = \"./weights/weights.ckpt\"\n",
    "# filename = path + \"train.h5\"\n",
    "# filename_test = path_test + \"test.h5\"\n",
    "# filename_pba = path_test + 'testPBA.h5'\n",
    "# with h5py.File(filename_test, 'r') as fh:\n",
    "#     testIDs = [*range(fh['geometry'].shape[-1])]\n",
    "\n",
    "# # Load normalization constants.\n",
    "# scaler = DataRescaler(path, filename=filename)\n",
    "# scaler.load(inputs=True, outputs=True)\n",
    "# scale = {\"y_min\":scaler.y_min, \"y_max\":scaler.y_max,\n",
    "#         \"x_min\":scaler.x_min, \"x_max\":scaler.x_max,\n",
    "#         \"e_min\":70, \"e_max\":220}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f22243e",
   "metadata": {
    "id": "bc62beca"
   },
   "source": [
    "## Define and load the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "858e42df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1622220733287,
     "user": {
      "displayName": "Oscar Pastor Serrano",
      "photoUrl": "",
      "userId": "05646652561813059794"
     },
     "user_tz": -120
    },
    "id": "9XGG1HuSIlmW",
    "outputId": "8ddf5dd9-8560-4414-d912-e4672d356a42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 19:04:59.058518: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-06 19:04:59.165022: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-06 19:04:59.166152: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-06 19:04:59.176159: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-06 19:04:59.179031: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-06 19:04:59.180524: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-06 19:05:05.571559: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-06 19:05:05.574127: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-06 19:05:05.574193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-01-06 19:05:05.576186: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-06 19:05:05.576781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2739 MB memory:  -> device: 0, name: Quadro P1000, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 150, 24, 24, 1)]     0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " conv_encoder (ConvEncoder)  (None, 151, 432)             189576    ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " transformer_encoder (Trans  (None, 151, 432)             1231977   ['conv_encoder[0][0]']        \n",
      " formerEncoder)                                           6                                       \n",
      "                                                                                                  \n",
      " conv_decoder (ConvDecoder)  (None, 150, 24, 24, 1)       123457    ['transformer_encoder[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12632809 (48.19 MB)\n",
      "Trainable params: 12632809 (48.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer = dota_energies(\n",
    "    num_tokens=param[\"num_tokens\"],\n",
    "    input_shape=param[\"data_shape\"],\n",
    "    projection_dim=param[\"projection_dim\"],\n",
    "    num_heads=param[\"num_heads\"],\n",
    "    num_transformers=param[\"num_transformers\"], \n",
    "    kernel_size=param[\"kernel_size\"],\n",
    "    causal=True\n",
    ")\n",
    "transformer.summary()\n",
    "\n",
    "# Load weights from checkpoint.\n",
    "#transformer.load_weights(path_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66dae0b",
   "metadata": {
    "id": "136ebcdd"
   },
   "source": [
    "## Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8289c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gamma results\n",
    "gamma_results = np.load('./utils/eval/gamma_analysis.npz')\n",
    "gamma_passrate = gamma_results[\"arr_1\"]\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(gamma_passrate[0], 20)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Mean: {np.mean(gamma_passrate[0])}\")\n",
    "print(f\"Stdev: {np.std(gamma_passrate[0])}\")\n",
    "print(f\"Min: {np.amin(gamma_passrate[0])}\")\n",
    "print(f\"Max: {np.amax(gamma_passrate[0])}\")\n",
    "\n",
    "# Print worst sample IDs\n",
    "gamma_IDs = gamma_results[\"arr_0\"]\n",
    "print(f\"Worst IDs: {gamma_IDs[0,:10]}\")\n",
    "print(f\"Worst pass rate: {gamma_passrate[0,8]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, prediction, ground_truth = infer(transformer, testIDs[1524], filename_test, scale)\n",
    "plot_beam(inputs, ground_truth, prediction,  gamma_evaluation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dcc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, prediction, ground_truth = infer(transformer, testIDs[2082], filename_test, scale)\n",
    "plot_slice(inputs, ground_truth, prediction, scale, savefig=True)\n",
    "inputs, prediction, ground_truth = infer(transformer, testIDs[1951], filename_test, scale)\n",
    "plot_slice(inputs, ground_truth, prediction, scale, savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2eb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, prediction, ground_truth = infer(transformer, testIDs[48], filename_test, scale)\n",
    "plot_slice(inputs, ground_truth, prediction, scale, savefig=True)\n",
    "inputs, prediction, ground_truth = infer(transformer, testIDs[1776], filename_test, scale)\n",
    "plot_slice(inputs, ground_truth, prediction, scale, savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c6387",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, prediction, ground_truth = from_file(filename_pba, testIDs[2082], filename_test)\n",
    "plot_slice(inputs, ground_truth, prediction, scale, cutoff=0.1, savefig=True)\n",
    "inputs, prediction, ground_truth = from_file(filename_pba, testIDs[1951], filename_test)\n",
    "plot_slice(inputs, ground_truth, prediction, scale, cutoff=0.1, savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948299c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, prediction, ground_truth = from_file(filename_pba, testIDs[48], filename_test)\n",
    "plot_slice(inputs, ground_truth, prediction, scale, cutoff=0.1, savefig=True)\n",
    "inputs, prediction, ground_truth = from_file(filename_pba, testIDs[1776], filename_test)\n",
    "plot_slice(inputs, ground_truth, prediction, scale, cutoff=0.1, savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f14865",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, prediction, ground_truth = from_file(filename_pba, testIDs[2082], filename_test)\n",
    "plot_slice(inputs, ground_truth, prediction, scale, cutoff=0.1, gamma_slice=False, savefig=True)\n",
    "inputs, prediction, ground_truth = infer(transformer, testIDs[2082], filename_test, scale)\n",
    "plot_slice(inputs, ground_truth, prediction, scale, cutoff=0.1, gamma_slice=False, savefig=True)\n",
    "inputs, prediction, ground_truth = from_file(filename_pba, testIDs[48], filename_test)\n",
    "plot_slice(inputs, ground_truth, prediction, scale, cutoff=0.1, gamma_slice=False, savefig=True)\n",
    "inputs, prediction, ground_truth = infer(transformer, testIDs[48], filename_test, scale)\n",
    "plot_slice(inputs, ground_truth, prediction, scale, cutoff=0.1, gamma_slice=False, savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5f4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dose_transformer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
